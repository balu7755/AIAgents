from langchain_core.language_models import BaseLanguageModel

def get_llm(config: dict) -> BaseLanguageModel:
    """
    Initialize and return the LLM (Ollama, ChatGPT, Groq, Grok, DeepSeek) based on config.yaml.
    """
    llm_cfg = config.get("llm", {})
    provider = llm_cfg.get("provider", "").strip().lower()
    model = llm_cfg.get("model", "").strip()
    temp = llm_cfg.get("temperature", 1.0)  # Default temperature

    if not provider or not model:
        raise ValueError("‚ùå LLM provider or model not specified in config['llm'].")

    # ‚úÖ Ollama
    if provider == "ollama":
        try:
            from langchain_ollama import ChatOllama
        except ImportError:
            raise ImportError("ü¶ô Install Ollama support with: pip install langchain-ollama")
        print(f"ü¶ô Using Ollama with model: '{model}', temperature: {temp}")
        return ChatOllama(model=model, temperature=temp)

    # ‚úÖ OpenAI ChatGPT
    elif provider in ("chatgpt", "openai"):
        try:
            from langchain_openai import ChatOpenAI
        except ImportError:
            raise ImportError("ü§ñ Install OpenAI support with: pip install langchain-openai")
        api_key = llm_cfg.get("api_key", "").strip()
        if not api_key:
            raise ValueError("‚ùå Missing OpenAI API key in config['llm']['api_key'].")
        print(f"ü§ñ Using OpenAI ChatGPT with model: '{model}', temperature: {temp}")
        return ChatOpenAI(model=model, api_key=api_key, temperature=temp)

    # ‚úÖ Groq LLM
    elif provider == "groq":
        try:
            from langchain_groq.chat_models import ChatGroq
        except ImportError:
            raise ImportError("‚ö° Install Groq support with: pip install langchain-groq")
        api_key = llm_cfg.get("api_key", "").strip()
        if not api_key:
            raise ValueError("‚ùå Missing Groq API key in config['llm']['api_key'].")
        print(f"‚ö° Using Groq LLM with model: '{model}', temperature: {temp}")
        return ChatGroq(model=model, api_key=api_key, temperature=temp)

    # ‚úÖ DeepSeek LLM
    elif provider == "deepseek":
        try:
            from langchain_deepseek import ChatDeepSeek
        except ImportError:
            raise ImportError("üß† Install DeepSeek support with: pip install langchain-deepseek")
        api_key = llm_cfg.get("api_key", "").strip()
        if not api_key:
            raise ValueError("‚ùå Missing DeepSeek API key in config['llm']['api_key'].")
        print(f"üß† Using DeepSeek LLM with model: '{model}', temperature: {temp}")
        return ChatDeepSeek(model=model, api_key=api_key, temperature=temp)

    # ‚ùå Unsupported provider
    else:
        raise ValueError(f"‚ùå Unsupported LLM provider: '{provider}'. Supported: ollama, openai, groq, grok, deepseek")
